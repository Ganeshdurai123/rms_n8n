---
phase: 07-request-books
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - server/package.json
  - server/src/modules/request/import.model.ts
  - server/src/modules/request/import.schema.ts
  - server/src/modules/request/import.service.ts
  - server/src/modules/request/import.controller.ts
  - server/src/modules/request/import.routes.ts
  - server/src/modules/request/request.routes.ts
  - server/src/modules/audit/auditLog.model.ts
autonomous: true
requirements:
  - BOOK-01
  - BOOK-02
  - BOOK-03
  - BOOK-04
  - BOOK-05

must_haves:
  truths:
    - "Uploaded Excel/CSV file is parsed and column headers are returned for mapping"
    - "Mapped data is validated row-by-row against program field definitions with per-row error details"
    - "Valid rows can be batch-created as draft requests in a single action"
    - "Import history records who imported, when, total rows, success count, and error count"
  artifacts:
    - path: "server/src/modules/request/import.model.ts"
      provides: "ImportJob Mongoose model for tracking import history"
      contains: "model ImportJob"
    - path: "server/src/modules/request/import.service.ts"
      provides: "File parsing, validation, batch creation, and import history logic"
      exports: ["parseUploadedFile", "validateImportRows", "executeBatchImport", "getImportHistory"]
    - path: "server/src/modules/request/import.controller.ts"
      provides: "HTTP handlers for upload, validate, execute import, and history"
      exports: ["upload", "validatePreview", "executeImport", "listHistory"]
    - path: "server/src/modules/request/import.routes.ts"
      provides: "Import route definitions mounted under request routes"
      contains: "router.post"
  key_links:
    - from: "server/src/modules/request/import.service.ts"
      to: "server/src/modules/request/request.service.ts"
      via: "reuses validateFields pattern for row validation"
      pattern: "validateFields"
    - from: "server/src/modules/request/import.routes.ts"
      to: "server/src/modules/request/request.routes.ts"
      via: "mounted as sub-resource under program requests"
      pattern: "router\\.use.*import"
    - from: "server/src/modules/request/import.service.ts"
      to: "server/src/modules/request/import.model.ts"
      via: "ImportJob.create for tracking"
      pattern: "ImportJob\\.(create|find)"
---

<objective>
Build the complete backend for Excel/CSV bulk import: file parsing, field mapping support, row-level validation, batch request creation, and import history tracking.

Purpose: Provides the server-side API that the frontend import wizard will call -- upload a file, get parsed columns, validate mapped data against program fields, execute batch import, and retrieve import history.

Output: ImportJob model, import service with parse/validate/execute/history functions, REST API endpoints, Zod schemas, and audit integration.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@server/src/modules/request/request.service.ts
@server/src/modules/request/request.model.ts
@server/src/modules/request/request.schema.ts
@server/src/modules/request/request.routes.ts
@server/src/modules/request/request.controller.ts
@server/src/modules/program/program.model.ts
@server/src/modules/audit/auditLog.model.ts
@server/src/modules/audit/audit.utils.ts
@server/package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: ImportJob model, Zod schemas, and xlsx dependency</name>
  <files>
    server/package.json
    server/src/modules/request/import.model.ts
    server/src/modules/request/import.schema.ts
    server/src/modules/audit/auditLog.model.ts
  </files>
  <action>
1. Install xlsx (SheetJS) for Excel/CSV parsing:
   ```bash
   cd server && npm install xlsx
   ```
   Note: xlsx is a pure-JS library with no native deps, works on all platforms. It handles both .xlsx and .csv formats.

2. Create `server/src/modules/request/import.model.ts`:
   - ImportJob Mongoose model with fields:
     - `programId` (ObjectId, ref: 'Program', required, indexed)
     - `performedBy` (ObjectId, ref: 'User', required)
     - `originalFilename` (String, required) -- the user's original file name
     - `status` (enum: 'pending', 'validated', 'completed', 'failed', default: 'pending')
     - `totalRows` (Number, required) -- total data rows in file (excluding header)
     - `successCount` (Number, default: 0) -- rows successfully imported
     - `errorCount` (Number, default: 0) -- rows that failed validation
     - `errors` (Array of { row: Number, field: String, message: String }) -- per-row error details
     - `columnMapping` (Map of String to String) -- maps file column names to program field keys
     - `parsedData` (Array of Mixed) -- the parsed+mapped row data (stored temporarily for validation preview)
     - timestamps: true
   - Export IMPORT_STATUSES as const array: ['pending', 'validated', 'completed', 'failed']
   - Add indexes: { programId: 1, createdAt: -1 }, { performedBy: 1 }

3. Create `server/src/modules/request/import.schema.ts`:
   - `uploadImportSchema`: validates multipart file upload params (programId from route params)
   - `validateImportSchema`: body with `importJobId` (objectId string) and `columnMapping` (Record<string, string> -- maps file column header names to program field definition keys). Also include `titleColumn` (string) for which file column maps to request title, and optional `descriptionColumn` (string).
   - `executeImportSchema`: body with `importJobId` (objectId string)
   - `listImportHistorySchema`: query with page (default 1), limit (default 20) for pagination
   - Export all types.

4. Update `server/src/modules/audit/auditLog.model.ts`:
   - Add 'import.created' to AUDIT_ACTIONS array
   - Add 'import' to AUDIT_ENTITY_TYPES array
   - This allows import operations to be tracked in the existing audit log system.
  </action>
  <verify>
    - `cd server && npx tsc --noEmit` compiles without errors
    - xlsx is in server/package.json dependencies
    - ImportJob model exports correctly
  </verify>
  <done>
    ImportJob Mongoose model exists with all required fields, Zod validation schemas cover all import endpoints, xlsx dependency installed, audit model extended with import action.
  </done>
</task>

<task type="auto">
  <name>Task 2: Import service (parse, validate, batch create, history) and API endpoints</name>
  <files>
    server/src/modules/request/import.service.ts
    server/src/modules/request/import.controller.ts
    server/src/modules/request/import.routes.ts
    server/src/modules/request/request.routes.ts
  </files>
  <action>
1. Create `server/src/modules/request/import.service.ts` with these exported functions:

   **parseUploadedFile(filePath: string, originalFilename: string, programId: string, userId: string):**
   - Use `xlsx.readFile(filePath)` to read the uploaded file (works for both .xlsx and .csv)
   - Extract the first sheet: `workbook.Sheets[workbook.SheetNames[0]]`
   - Convert to JSON with `xlsx.utils.sheet_to_json(sheet, { header: 1 })` to get array-of-arrays
   - First row = column headers (strings). Remaining rows = data.
   - Create an ImportJob document with status='pending', totalRows = data rows count, parsedData = raw rows as array of objects keyed by column headers, originalFilename
   - Return: { importJobId, columns: string[] (header names), sampleRows: first 5 data rows as objects, totalRows }
   - Clean up the temp file after parsing with fs.unlink (fire-and-forget)

   **validateImportRows(importJobId: string, columnMapping: Record<string, string>, titleColumn: string, descriptionColumn?: string):**
   - Load ImportJob by ID, verify status is 'pending'
   - Load the program's fieldDefinitions via getProgramById(importJob.programId)
   - For each row in importJob.parsedData:
     - Map file columns to program fields using columnMapping
     - Extract title from titleColumn, description from descriptionColumn
     - Validate title exists and is 3-200 chars
     - For each mapped field, validate type/required/options using the same logic as validateFields in request.service.ts (BUT collect errors instead of throwing):
       - text: must be string
       - number: must parse to number (use parseFloat, check isNaN)
       - date: must be valid date string
       - dropdown: must be in options array
       - checkbox: must be 'true'/'false'/'yes'/'no'/1/0 (cast to boolean)
       - file_upload: skip for import (cannot bulk-import files)
     - Collect errors as { row: rowIndex+1, field: fieldKey, message: string }
   - Update ImportJob: status='validated', columnMapping, errors array, errorCount
   - Return: { importJobId, totalRows, validCount, errorCount, errors (first 100), validRows (preview of first 5 valid mapped rows with title/description/fields) }

   **executeBatchImport(importJobId: string, userId: string, userRole: Role):**
   - Load ImportJob by ID, verify status is 'validated'
   - Load program for field definitions and timeframe check
   - Filter to only valid rows (skip rows that have errors by row index)
   - Use Request.insertMany() for bulk creation: for each valid row, build document with:
     - programId, title (from mapped titleColumn), description (from mapped descriptionColumn), fields (mapped and type-coerced), status: 'draft', createdBy: userId, priority: 'medium'
   - Set insertMany ordered:false so partial failures don't stop the whole batch
   - Update ImportJob: status='completed', successCount = inserted count
   - Create a single audit entry: action='import.created', entityType='import', with metadata: { importJobId, totalRows, successCount, errorCount }
   - Invalidate request list cache
   - Emit Socket.IO event to program room for real-time update (optional, fire-and-forget)
   - Return: { importJobId, successCount, errorCount, totalRows }

   **getImportHistory(programId: string, page: number, limit: number):**
   - Query ImportJob.find({ programId }).sort({ createdAt: -1 }).skip().limit().populate('performedBy', 'firstName lastName email')
   - Return: { imports, total, page, limit }

2. Create `server/src/modules/request/import.controller.ts`:
   - `upload`: POST handler. Uses multer single file upload (field name 'file'). Accepts .xlsx, .xls, .csv MIME types. Max 10MB. Calls parseUploadedFile with req.file.path, req.file.originalname, req.params.programId, req.user._id. Returns 200 with parsed result.
   - `validatePreview`: POST handler. Calls validateImportRows with req.body. Returns 200 with validation result.
   - `executeImport`: POST handler. Calls executeBatchImport with req.body.importJobId, req.user._id, req.user.role. Returns 201 with import result.
   - `listHistory`: GET handler. Calls getImportHistory with req.params.programId, req.query.page, req.query.limit. Returns 200 with paginated response.

3. Create `server/src/modules/request/import.routes.ts`:
   - Router with mergeParams: true
   - All routes require authenticate + authorizeProgram()
   - Configure multer: diskStorage to 'uploads/imports/' directory, fileFilter for xlsx/xls/csv MIME types + .csv extension check (CSV files sometimes have text/plain MIME type), limits: { fileSize: 10 * 1024 * 1024 }
   - POST '/' -- upload file (multer.single('file') middleware + upload controller)
   - POST '/validate' -- validate mapped data (validate(validateImportSchema) + validatePreview controller)
   - POST '/execute' -- execute batch import (validate(executeImportSchema) + executeImport controller). Restrict to admin/manager: authorizeProgram({ roles: ['manager'] })
   - GET '/history' -- list import history (validate(listImportHistorySchema, 'query') + listHistory controller)

4. Update `server/src/modules/request/request.routes.ts`:
   - Import importRouter from './import.routes.js'
   - Mount BEFORE /:requestId routes: `router.use('/import', importRouter)` -- this prevents Express misparse of 'import' as a requestId (same pattern as 'export' route)
  </action>
  <verify>
    - `cd server && npx tsc --noEmit` compiles without errors
    - Routes are correctly mounted: POST /import, POST /import/validate, POST /import/execute, GET /import/history
    - Verify multer config accepts xlsx, xls, and csv file types
  </verify>
  <done>
    Complete backend API for bulk import: file upload parses Excel/CSV and returns column headers, validation endpoint checks rows against program field definitions with per-row errors, execute endpoint batch-creates draft requests, history endpoint returns paginated import jobs. All integrated with audit logging, cache invalidation, and route middleware chain.
  </done>
</task>

</tasks>

<verification>
1. TypeScript compilation: `cd server && npx tsc --noEmit` passes
2. Import routes are accessible at /api/v1/programs/:programId/requests/import/*
3. ImportJob model correctly stores import metadata and parsed data
4. Audit log model accepts 'import.created' action and 'import' entity type
5. xlsx dependency is installed and importable
</verification>

<success_criteria>
- POST /import accepts .xlsx/.csv files up to 10MB, parses them, returns column headers and sample data
- POST /import/validate accepts column mapping, validates all rows against program fields, returns per-row errors
- POST /import/execute creates draft requests for all valid rows, tracks success/error counts
- GET /import/history returns paginated import job records for a program
- All operations create appropriate audit log entries
</success_criteria>

<output>
After completion, create `.planning/phases/07-request-books/07-01-SUMMARY.md`
</output>
